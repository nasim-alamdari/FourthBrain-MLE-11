{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-egkpUTSarLK"
   },
   "source": [
    "# In Class Follow Along Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJjtovQ9axry"
   },
   "source": [
    "First things first, we'll set-up the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2i_bITLVY7zD"
   },
   "outputs": [],
   "source": [
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OkIZ8Z6oZEKp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cleaned_tweets = pd.read_csv(\"cleaned_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wP0EBj57ZZiw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when a father is dysfunctional and is so sel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks for #lyft credit i cant use cause the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tidy_tweet  label\n",
       "0    when a father is dysfunctional and is so sel...      0\n",
       "1    thanks for #lyft credit i cant use cause the...      0\n",
       "2                                bihday your majesty      0\n",
       "3  #model   i love u take with u all the time in ...      0\n",
       "4              factsguide society now    #motivation      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YNdI8lPqZfBK"
   },
   "outputs": [],
   "source": [
    "X, y = pd.Series(cleaned_tweets['tidy_tweet']), pd.Series(cleaned_tweets['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mckqeu1aZpfN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_sub, X_test, y_train_sub, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VTWa4CbkaAeD"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sub, y_train_sub, test_size=0.2, stratify=y_train_sub, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "990gUduTa2a5"
   },
   "source": [
    "## Positional Embedding Layer\n",
    "\n",
    "We'll make the positional embedding layer as seen in the \"Attention is all you need\" paper!\n",
    "\n",
    "The idea behind Positional Encoding is fairly simple as well: to give the model access to token order information, therefore we are going to add the token's position in the sentence to each word embedding.\n",
    "\n",
    "Thus, **one input word embedding will have two components**: the usual token vector representing the token independent of any specific context, and a position vector representing the position of the token in the current sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CPEA_KsZaky5"
   },
   "outputs": [],
   "source": [
    "### Positional Embedding\n",
    "from tensorflow.keras import layers as L\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class PositionalEmbedding(L.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        self.token_embeddings =   L.Embedding(input_dim=input_dim, output_dim=output_dim ) # YOUR CODE HERE\n",
    "        self.position_embeddings =  L.Embedding(input_dim=sequence_length, output_dim=output_dim ) # YOUR CODE HERE\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzCOplI_eJEf"
   },
   "source": [
    "## Transformer Block\n",
    "\n",
    "Recently most of the natural language processing tasks are being dominated by the Transformer architecture, introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762), which used a simple mechanism called Neural Attention as one of its building blocks. As the title suggests this architecture didn't require any recurrent layer. We now build a text classification using Attention and Positional Embeddings.\n",
    "\n",
    "Transformer (attention) Block.\n",
    "\n",
    "The concept of Neural Attention is fairly simple; i.e., not all input information seen by a model is equally important to the task at hand. Although this concept has been utilized at various different places as well, e.g., max pooling in ConvNets, but the kind of attention we are looking for should be context aware.\n",
    "\n",
    "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other; in other words, calculate attention of all other inputs with respect to one input.\n",
    "\n",
    "In the paper, the authors proposed another type of attention mechanism called multi-headed attention which refers to the fact that the outer space of the self attention layer gets factored into a set of independent sub-spaces learned separately, where each subspace is called a \"head\".**You need to implement the multi-head attention layer, supplying values for two parameters: num_heads and key_dim.**\n",
    "\n",
    "There is a learnable dense projection present after the multi-head attention which enables the layer to actually learn something, as opposed to being a purely stateless transformation. You need to implement dense_proj, use the tf.keras.Sequential to stack two dense layers:\n",
    "\n",
    " 1. first dense layer with `dense_dim` units and activation function `relu`;\n",
    " 2. second dense layer with `embed_dim` units and no activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mSQCJSvTecQ0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This multihead attention first projects query, key and value. \n",
    "These are (effectively) a list of tensors of length num_attention_heads, \n",
    "where the corresponding shapes are \n",
    "    - (batch_size, <query dimensions>, key_dim), \n",
    "    - (batch_size, <key/value dimensions>, key_dim), \n",
    "    - (batch_size, <key/value dimensions>, value_dim).\n",
    "    \n",
    "num_heads: Number of attention heads.\n",
    "key_dim: Size of each attention head for query and key.\n",
    "\n",
    "Masking allows us to mask words where attention is not required. \n",
    "This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
    "It is mostly required in decoder where you want attention w.r.t to previous timesteps, not future ones.\n",
    "\"\"\"\n",
    "class TransformerBlock(L.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention =  L.MultiHeadAttention(num_heads, key_dim=embed_dim)# YOUR CODE HERE\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            L.Dense(dense_dim, activation='relu'),\n",
    "            L.Dense(embed_dim)\n",
    "            ])\n",
    "        self.layernorm1 = L.LayerNormalization()\n",
    "        self.layernorm2 = L.LayerNormalization()\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[: tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm1(inputs + attention_output) # add and norm\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm2(proj_input + proj_output) # add and norm\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGlaqhELc4XJ"
   },
   "source": [
    "## Transformer Model in Keras\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kGU1mvM5dBCR"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10_000\n",
    "EMBED_DIM = 256\n",
    "DENSE_DIM = 32\n",
    "NUM_HEADS = 2\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FQNdZcOdEwf"
   },
   "source": [
    "Tokenizer.\n",
    "\n",
    "The tokenizer is a simple tool to convert a text into a sequence of tokens. **It is used to convert the training data into a sequence of integers, which are then used as input to the model.**\n",
    "\n",
    "Use Tokenizer to create a tokenizer for the training data. Set the num_words parameter to the number of words to keep in the vocabulary, and oov_token to be \"\\<unk>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FpvW57zwdCrW"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token =\"<unk>\")# oov_token =\"<unk>\" mean if no vocab, we willl replace it with \"<unk>\"\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qJJ073cdMuF"
   },
   "source": [
    "Pad the sequences.\n",
    "\n",
    "The tokenizer outputs a sequence of integers, which are then used as input to the model. However, the model expects a sequence of fixed length. To pad the sequences to the same length, use sequence.pad_sequences from keras.preprocessing.\n",
    "\n",
    "Complete function preprocess below to 1) tokenize the texts 2) pad the sequences to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ABEZtL1fdNVP"
   },
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "def preprocess(texts, tokenizer, maxlen:int = MAX_LEN):\n",
    "    seqs = tokenizer.texts_to_sequences(texts) # YOUR CODE HERE\n",
    "    tokenized_text =  pad_sequences(seqs, maxlen= MAX_LEN) # YOUR CODE HERE\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFMp4gygdQXr"
   },
   "source": [
    "Preprocess the data.\n",
    "\n",
    "Use preprocess to preprocess the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M_64X4SUdRWt"
   },
   "outputs": [],
   "source": [
    "X_train = preprocess(X_train, tokenizer, maxlen= MAX_LEN) # YOUR CODE HERE\n",
    "X_valid = preprocess(X_valid, tokenizer, maxlen= MAX_LEN) # YOUR CODE HERE\n",
    "X_test  = preprocess(X_test, tokenizer, maxlen= MAX_LEN) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17898, 256), (4475, 256), (9589, 256))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEw_iUdLdVod"
   },
   "source": [
    "Define the model with the following architecture:\n",
    "\n",
    "* Input Layer\n",
    "* Positional Embeddings\n",
    "* Transformer Block\n",
    "* Pooling\n",
    "* Dropout\n",
    "* Output Layer\n",
    "\n",
    "If you are not familiar with keras functional API, take a read [here](https://keras.io/guides/functional_api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nj6VLLiRdW3u"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "x = PositionalEmbedding(MAX_LEN, VOCAB_SIZE, EMBED_DIM)(inputs) # YOUR CODE HERE\n",
    "x = TransformerBlock(EMBED_DIM, DENSE_DIM, NUM_HEADS)(x) # YOUR CODE HERE\n",
    "x = L.GlobalMaxPooling1D()(x)\n",
    "x = L.Dropout(0.1)(x)\n",
    "outputs = L.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vBkk6dAdox1"
   },
   "source": [
    "Compile model.\n",
    "\n",
    "Use 'adam' for the optimizer and accuracy for metrics, supply the correct value for loss.\n",
    "\n",
    "Remember, this is a binary classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "D0H-JOY7dpa8"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', # YOUR CODE HERE\n",
    "    loss='binary_crossentropy', # YOUR CODE HERE\n",
    "    metrics=['accuracy']) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u_noMiW2dss4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posit  (None, None, 256)        2625536   \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, None, 256)        543776    \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,169,569\n",
      "Trainable params: 3,169,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mf4WnvTdu2s"
   },
   "source": [
    "Add [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) and [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to stop training if the model does not improve a set metric after a given number of epochs.\n",
    "\n",
    "Create an EarlyStopping object named es to stop training if the validation loss does not improve after 5 epochs. Set verbose to display messages when the callback takes an action and set restore_best_weights to restore model weights from the epoch with the best value of the monitored metric.\n",
    "\n",
    "Use ReduceLROnPlateau to reduce the learning rate if the validation loss does not improve after 3 epochs. Set verbose to display messages when the callback takes an action and use default values for other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "A221divwd2pX"
   },
   "outputs": [],
   "source": [
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, restore_best_weights=True) # YOUR CODE HERE\n",
    "rlp =  keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",patience=3, verbose=1)# Reduce learning rate when a metric has stopped improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prI5VSd5d5ju"
   },
   "source": [
    "Train the model.\n",
    "\n",
    "Supply both EarlyStopping and ReduceLROnPlateau for callbacks. Set epochs to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rxTZtZ_nd8Cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "560/560 [==============================] - 168s 298ms/step - loss: 0.1904 - accuracy: 0.9405 - val_loss: 0.1403 - val_accuracy: 0.9528 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "560/560 [==============================] - 185s 330ms/step - loss: 0.0721 - accuracy: 0.9752 - val_loss: 0.1308 - val_accuracy: 0.9580 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "560/560 [==============================] - 183s 326ms/step - loss: 0.0240 - accuracy: 0.9926 - val_loss: 0.1575 - val_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "560/560 [==============================] - 184s 328ms/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 0.1574 - val_accuracy: 0.9560 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "560/560 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "560/560 [==============================] - 190s 340ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.1867 - val_accuracy: 0.9571 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "560/560 [==============================] - 225s 403ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.1994 - val_accuracy: 0.9578 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "560/560 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998Restoring model weights from the end of the best epoch: 2.\n",
      "560/560 [==============================] - 267s 476ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2054 - val_accuracy: 0.9578 - lr: 1.0000e-04\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks = [es, rlp],# YOUR CODE HERE\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCs1ljq5d-fl"
   },
   "source": [
    "Evaluate the trained model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk20ucAWd_hZ"
   },
   "outputs": [],
   "source": [
    "y_pred = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vpr4AktheCHj"
   },
   "source": [
    "Visualize both loss and accuracy curves for the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UQh-ADKeDUD"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(7)\n",
    "print(\"The results are being visualized\")\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
